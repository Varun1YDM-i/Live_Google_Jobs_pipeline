{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa0adab4-8c86-4dac-838e-bf1e80bf1c2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import trim, upper, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e63d7d08-29f6-4956-b3be-af3df9877899",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# defining the Spark Session named Google_JobETL_Bronze\n",
    "app_name = \"Google_JobETL_Silver\"\n",
    "spark = SparkSession.builder.appName(app_name).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e841d0e6-99df-4184-a6bc-eab0f2d0dd52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# checking spark Session name (appname)\n",
    "print('App Name :',app_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8076f18c-6a75-4d91-bee1-be7a71684f8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Checking all the catalogs in unity-catalog\n",
    "display(spark.sql('SHOW CATALOGS'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12f1ffd5-9361-47f0-ae6e-b80c4187c9af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Checking all the avilable schemas in the catalog\n",
    "display( spark.sql('SHOW SCHEMAS') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0414c374-c49d-4d5d-bafb-f73605a599de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Checking the current unity-catalog and creating catalog if not present and using it\n",
    "spark.sql(' CREATE CATALOG IF NOT EXISTS job_marketplace ')\n",
    "spark.sql(' USE CATALOG job_marketplace ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d71c562-2653-4fdb-a25d-7d004c73f493",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# displaying  present schemas in the job_marketplace catalog\n",
    "display( spark.sql('SHOW SCHEMAS') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b216955-a184-4b8a-99d2-5ace84da0ea8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# checking and creating the silver_layer schema in the job_marketplace catalog\n",
    "spark.sql('CREATE SCHEMA IF NOT EXISTS job_marketplace.silver_layer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edb7452c-47b5-46fd-90e0-ecb0ef962007",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Checking if the silver layer schema is present in the job_marketplace catalog\n",
    "display( spark.sql('SHOW SCHEMAS') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9de799ce-5407-4328-8e88-f97c0b630cfc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reading the data from bronze layer\n",
    "df = spark.read.format('delta').table('job_marketplace.bronze_layer.daily_jobs')\n",
    "display(df)\n",
    "print(\"✅Reading from Bronze Layer --> Silver Layer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a75cb54-f098-48ff-8415-d79aaad93cf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "columns_to_extract = [\"posted_at\", \"schedule_type\"] \n",
    "\n",
    "for key in columns_to_extract:\n",
    "    df = df.withColumn(key, expr(f\"detected_extensions['{key}']\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4220d124-82ab-4ebe-a2e7-6d36d53ccb92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display( df )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f238816-f740-4d99-9637-ef68483c22c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import regexp_replace, col , lower , split, trim, size, when\n",
    "\n",
    "# df_cleaned = df.withColumn(\n",
    "#     \"location_cleaned\",\n",
    "#     regexp_replace(col(\"location\"), r\"\\s*\\(\\+\\d+\\s+others?\\)\", \"\")\n",
    "# )\n",
    "\n",
    "# split_col = split(col(\"location_cleaned\"), \",\")\n",
    "\n",
    "# df_split = df_cleaned.withColumn(\"city_raw\", trim(split_col.getItem(0))) \\\n",
    "#     .withColumn(\"state_raw\", when(size(split_col) >= 2, trim(split_col.getItem(1)))) \\\n",
    "#     .withColumn(\"country_raw\", when(size(split_col) >= 3, trim(split_col.getItem(2))))  # null-safe\n",
    "\n",
    "# df_final = df_split.withColumn(\n",
    "#     \"city\",\n",
    "#     when(lower(col(\"location_cleaned\")).isin(\"india\", \"anywhere\"), None).otherwise(col(\"city_raw\"))\n",
    "# ).withColumn(\n",
    "#     \"state\",\n",
    "#     when(lower(col(\"location_cleaned\")).isin(\"india\", \"anywhere\"), None).otherwise(col(\"state_raw\"))\n",
    "# ).withColumn(\n",
    "#     \"country\",\n",
    "#     when(lower(col(\"location_cleaned\")) == \"anywhere\", \"Remote\")\n",
    "#     .when(col(\"country_raw\").isNull(), \"India\")\n",
    "#     .otherwise(col(\"country_raw\"))\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3d80466-f7b7-4aa5-98b4-d0de89bdd7b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, col, lower, split, trim, size, when\n",
    "\n",
    "# ✅ Step 1: Clean (+N others) from location\n",
    "df_cleaned = df.withColumn(\n",
    "    \"location_cleaned\",\n",
    "    regexp_replace(col(\"location\"), r\"\\s*\\(\\+\\d+\\s+others?\\)\", \"\")\n",
    ")\n",
    "\n",
    "# ✅ Step 2: Split location into components\n",
    "split_col = split(col(\"location_cleaned\"), \",\")\n",
    "\n",
    "# ✅ Step 3: Get raw parts\n",
    "df_split = df_cleaned.withColumn(\"city_raw\", trim(split_col.getItem(0))) \\\n",
    "    .withColumn(\"state_raw\", when(size(split_col) >= 2, trim(split_col.getItem(1)))) \\\n",
    "    .withColumn(\"country_raw\", when(size(split_col) >= 3, trim(split_col.getItem(2))))  # null-safe\n",
    "\n",
    "# ✅ Step 4: List of known Indian states (lowercase)\n",
    "known_states = [\n",
    "    \"andhra pradesh\", \"arunachal pradesh\", \"assam\", \"bihar\", \"chhattisgarh\", \"goa\",\n",
    "    \"gujarat\", \"haryana\", \"himachal pradesh\", \"jharkhand\", \"karnataka\", \"kerala\",\n",
    "    \"madhya pradesh\", \"maharashtra\", \"manipur\", \"meghalaya\", \"mizoram\", \"nagaland\",\n",
    "    \"odisha\", \"punjab\", \"rajasthan\", \"sikkim\", \"tamil nadu\", \"telangana\", \"tripura\",\n",
    "    \"uttar pradesh\", \"uttarakhand\", \"west bengal\", \"delhi\", \"jammu and kashmir\"\n",
    "]\n",
    "\n",
    "# ✅ Step 5: Final transformations\n",
    "df_final = df_split.withColumn(\n",
    "    \"city\",\n",
    "    when(lower(col(\"location_cleaned\")).isin(\"india\", \"anywhere\"), None)\n",
    "    .when(lower(col(\"city_raw\")).isin(known_states) & \n",
    "          (lower(col(\"state_raw\")).isin([\"india\"]) | col(\"state_raw\").isNull()), None)\n",
    "    .otherwise(col(\"city_raw\"))\n",
    ").withColumn(\n",
    "    \"state\",\n",
    "    when(lower(col(\"location_cleaned\")).isin(\"india\", \"anywhere\"), None)\n",
    "    .when(lower(col(\"city_raw\")).isin(known_states) & \n",
    "          (lower(col(\"state_raw\")).isin([\"india\"]) | col(\"state_raw\").isNull()), col(\"city_raw\"))\n",
    "    .otherwise(col(\"state_raw\"))\n",
    ").withColumn(\n",
    "    \"country\",\n",
    "    when(lower(col(\"location_cleaned\")) == \"anywhere\", \"Remote\")\n",
    "    .when(col(\"country_raw\").isNotNull(), col(\"country_raw\"))\n",
    "    .when(col(\"state_raw\").isNotNull(), col(\"state_raw\"))\n",
    "    .otherwise(\"India\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03cd982c-916b-4302-ba1d-9d9d11382fed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display( df_final )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f4b4dd9-ba55-4136-968a-0a139cc28792",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_final\n",
    "from pyspark.sql.functions import regexp_extract, col, current_timestamp, expr, when, lower\n",
    "\n",
    "# Extract the number and unit\n",
    "df1 = df_final.withColumn(\"posted_at_lower\", lower(col(\"posted_at\")))\n",
    "\n",
    "df2 = df1.withColumn(\"number\", regexp_extract(\"posted_at_lower\", r\"(\\d+)\", 1).cast(\"int\")) \\\n",
    "         .withColumn(\"unit\", regexp_extract(\"posted_at_lower\", r\"\\d+\\s*(\\w+)\", 1))\n",
    "\n",
    "# Create expressions for timestamp subtraction using expr and string interpolation\n",
    "df_final1 = df2.withColumn(\n",
    "    \"posted_at_ts\",\n",
    "    when(col(\"unit\").isin(\"hour\", \"hours\"),\n",
    "         expr(\"timestampadd(HOUR, -number, current_timestamp())\"))\n",
    "    .when(col(\"unit\").isin(\"day\", \"days\"),\n",
    "         expr(\"timestampadd(DAY, -number, current_timestamp())\"))\n",
    "    .when(col(\"unit\").isin(\"minute\", \"minutes\"),\n",
    "         expr(\"timestampadd(MINUTE, -number, current_timestamp())\"))\n",
    ")\n",
    "\n",
    "df_final2 = df.withColumn(\"posted_at_ts\", when(col(\"posted_at_ts\").isNull(), current_timestamp()).otherwise(col(\"posted_at_ts\")))\n",
    "\n",
    "# Show result\n",
    "display(df_final2.select(\"posted_at\", \"number\", \"unit\", \"posted_at_ts\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dbcc43c-d2ab-48ce-9797-51246c7e1b5c",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{\"share_link\":{\"format\":{\"preset\":\"string-preset-url\"}}}},\"syncTimestamp\":1752128420649}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "silver_df = df_final1.selectExpr( \"job_id\" , \"title as job_title\" , \"search_role\", \"company_name\", \"description\", \"location_cleaned as loacation\", 'city' , 'state','country', \"share_link\", \"via\", \"posted_at_ts as posted_at\", \"schedule_type\")\n",
    "\n",
    "display( silver_df )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "198e5fd6-79de-43ae-9638-c05a100ed6c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a temp view to use in merge\n",
    "silver_df.createOrReplaceTempView(\"new_jobs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d4a4c69-ca3d-4df6-809a-1b4225af297c",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{\"share_link\":{\"format\":{\"preset\":\"string-preset-url\"}}}},\"syncTimestamp\":1752130032501}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from new_jobs LIMIT 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d596ff47-ab8f-497c-be8c-508577051104",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS job_marketplace.silver_layer.clean_jobs (\n",
    "  job_id STRING,\n",
    "  job_title STRING,\n",
    "  search_role STRING,\n",
    "  company_name STRING,\n",
    "  description STRING,\n",
    "  loacation STRING,           -- Note: 'location' is misspelled in the source; keeping it consistent\n",
    "  city STRING,\n",
    "  state STRING,\n",
    "  country STRING,\n",
    "  share_link STRING,\n",
    "  via STRING,\n",
    "  posted_at TIMESTAMP,        -- You now have it in proper timestamp format\n",
    "  schedule_type STRING\n",
    ")\n",
    "USING DELTA;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e12cb6ae-fb2f-4b15-8dd6-b92c6ab0bb34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- DROP TABLE job_marketplace.silver_layer.clean_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9735dc01-c13e-4254-bd33-9d19b66d3bc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from job_marketplace.silver_layer.clean_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d9c593a-ea21-4da0-b243-10034025a5ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, col\n",
    "\n",
    "# Deduplicate source rows by keeping the latest posted_at\n",
    "window_spec = Window.partitionBy(\"job_id\").orderBy(col(\"posted_at\").desc())\n",
    "\n",
    "new_jobs_deduped = silver_df.withColumn(\"rn\", row_number().over(window_spec)) \\\n",
    "                             .filter(col(\"rn\") == 1) \\\n",
    "                             .drop(\"rn\")\n",
    "\n",
    "# Register deduped DataFrame as temp view\n",
    "new_jobs_deduped.createOrReplaceTempView(\"new_jobs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d18bd734-fd22-4694-9920-33e2dac17416",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "merge_sql = \"\"\"\n",
    "MERGE INTO job_marketplace.silver_layer.clean_jobs AS target\n",
    "USING new_jobs AS source\n",
    "ON target.job_id = source.job_id\n",
    "WHEN MATCHED THEN UPDATE SET\n",
    "  target.job_title      = source.job_title,\n",
    "  target.search_role    = source.search_role,\n",
    "  target.company_name   = source.company_name,\n",
    "  target.description    = source.description,\n",
    "  target.loacation      = source.loacation,\n",
    "  target.city           = source.city,\n",
    "  target.state          = source.state,\n",
    "  target.country        = source.country,\n",
    "  target.share_link     = source.share_link,\n",
    "  target.via            = source.via,\n",
    "  target.posted_at      = source.posted_at,\n",
    "  target.schedule_type  = source.schedule_type\n",
    "WHEN NOT MATCHED THEN INSERT (\n",
    "  job_id,\n",
    "  job_title,\n",
    "  search_role,\n",
    "  company_name,\n",
    "  description,\n",
    "  loacation,\n",
    "  city,\n",
    "  state,\n",
    "  country,\n",
    "  share_link,\n",
    "  via,\n",
    "  posted_at,\n",
    "  schedule_type\n",
    ") VALUES (\n",
    "  source.job_id,\n",
    "  source.job_title,\n",
    "  source.search_role,\n",
    "  source.company_name,\n",
    "  source.description,\n",
    "  source.loacation,\n",
    "  source.city,\n",
    "  source.state,\n",
    "  source.country,\n",
    "  source.share_link,\n",
    "  source.via,\n",
    "  source.posted_at,\n",
    "  source.schedule_type\n",
    ")\n",
    "\"\"\"\n",
    "spark.sql(merge_sql)\n",
    "print(\"✅ Data successfully merged into Silver Layer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b5ba6bd-fc07-49ba-91ee-86e003c5cd78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from job_marketplace.silver_layer.clean_jobs limit 1"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5410573655789474,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Silver_Layer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
